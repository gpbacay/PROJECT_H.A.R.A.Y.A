Metadata-Version: 2.1
Name: openvino-dev
Version: 2023.0.1
Summary: OpenVINO(TM) Development Tools
Home-page: https://docs.openvino.ai/2023.0/index.html
Download-URL: https://github.com/openvinotoolkit/openvino/tags
Author: Intel® Corporation
Author-email: openvino_pushbot@intel.com
License: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Description-Content-Type: text/markdown
License-File: readme.txt
License-File: LICENSE
License-File: dev-third-party-programs.txt
License-File: omz-third-party-programs.txt
Requires-Dist: addict (>=2.4.0)
Requires-Dist: defusedxml (>=0.7.1)
Requires-Dist: jstyleson (>=0.0.2)
Requires-Dist: networkx (<=2.8.8)
Requires-Dist: networkx (<=3.1)
Requires-Dist: numpy (>=1.16.6)
Requires-Dist: opencv-python
Requires-Dist: opencv-python (>=4.5)
Requires-Dist: openvino-telemetry (>=2022.1.0)
Requires-Dist: pillow (>=8.1.2)
Requires-Dist: pyyaml (>=5.4.1)
Requires-Dist: requests (>=2.25.1)
Requires-Dist: texttable (>=1.6.3)
Requires-Dist: tqdm (>=4.54.1)
Requires-Dist: openvino (==2023.0.1)
Requires-Dist: importlib-metadata ; python_version < "3.8" and sys_platform == "win32"
Requires-Dist: scipy (~=1.7) ; python_version == "3.7"
Requires-Dist: scipy (>=1.8) ; python_version >= "3.8"
Provides-Extra: caffe
Requires-Dist: protobuf (<4.0.0,>=3.18.1) ; extra == 'caffe'
Requires-Dist: networkx (<=2.8.8) ; extra == 'caffe'
Requires-Dist: requests (>=2.25.1) ; extra == 'caffe'
Requires-Dist: defusedxml (>=0.7.1) ; extra == 'caffe'
Requires-Dist: numpy (<1.25.0,>=1.16.6) ; extra == 'caffe'
Requires-Dist: fastjsonschema (<2.17,>=2.15.1) ; extra == 'caffe'
Provides-Extra: kaldi
Requires-Dist: networkx (<=2.8.8) ; extra == 'kaldi'
Requires-Dist: requests (>=2.25.1) ; extra == 'kaldi'
Requires-Dist: defusedxml (>=0.7.1) ; extra == 'kaldi'
Requires-Dist: numpy (<1.25.0,>=1.16.6) ; extra == 'kaldi'
Requires-Dist: fastjsonschema (<2.17,>=2.15.1) ; extra == 'kaldi'
Provides-Extra: mxnet
Requires-Dist: networkx (<=2.8.8) ; extra == 'mxnet'
Requires-Dist: requests (>=2.25.1) ; extra == 'mxnet'
Requires-Dist: defusedxml (>=0.7.1) ; extra == 'mxnet'
Requires-Dist: numpy (<1.25.0,>=1.16.6) ; extra == 'mxnet'
Requires-Dist: fastjsonschema (<2.17,>=2.15.1) ; extra == 'mxnet'
Requires-Dist: urllib3 (>=1.26.4) ; extra == 'mxnet'
Requires-Dist: mxnet (<=1.9.1,>=1.7.0.post2) ; (sys_platform != "win32") and extra == 'mxnet'
Requires-Dist: mxnet (~=1.2.0) ; (sys_platform == "win32") and extra == 'mxnet'
Provides-Extra: onnx
Requires-Dist: protobuf (<4.0.0,>=3.18.1) ; extra == 'onnx'
Requires-Dist: networkx (<=2.8.8) ; extra == 'onnx'
Requires-Dist: requests (>=2.25.1) ; extra == 'onnx'
Requires-Dist: onnx (<=1.13.1,>=1.8.1) ; extra == 'onnx'
Requires-Dist: defusedxml (>=0.7.1) ; extra == 'onnx'
Requires-Dist: numpy (<1.25.0,>=1.16.6) ; extra == 'onnx'
Requires-Dist: fastjsonschema (<2.17,>=2.15.1) ; extra == 'onnx'
Provides-Extra: paddle
Requires-Dist: paddlepaddle (>=2.2.0) ; extra == 'paddle'
Provides-Extra: pytorch
Requires-Dist: onnx (>=1.13) ; extra == 'pytorch'
Requires-Dist: torchvision (>=0.9.1) ; extra == 'pytorch'
Requires-Dist: torch (>=1.8.1) ; extra == 'pytorch'
Requires-Dist: yacs (>=0.1.8) ; extra == 'pytorch'
Requires-Dist: scipy (>=1.5.4) ; extra == 'pytorch'
Provides-Extra: tensorflow
Requires-Dist: tensorflow (<2.13.0,>=1.15.5) ; extra == 'tensorflow'
Requires-Dist: networkx (<=2.8.8) ; extra == 'tensorflow'
Requires-Dist: requests (>=2.25.1) ; extra == 'tensorflow'
Requires-Dist: defusedxml (>=0.7.1) ; extra == 'tensorflow'
Requires-Dist: numpy (<1.25.0,>=1.16.6) ; extra == 'tensorflow'
Requires-Dist: fastjsonschema (<2.17,>=2.15.1) ; extra == 'tensorflow'
Provides-Extra: tensorflow2
Requires-Dist: networkx (<=2.8.8) ; extra == 'tensorflow2'
Requires-Dist: requests (>=2.25.1) ; extra == 'tensorflow2'
Requires-Dist: defusedxml (>=0.7.1) ; extra == 'tensorflow2'
Requires-Dist: tensorflow (<2.13.0,>=2.5) ; extra == 'tensorflow2'
Requires-Dist: numpy (<1.25.0,>=1.16.6) ; extra == 'tensorflow2'
Requires-Dist: fastjsonschema (<2.17,>=2.15.1) ; extra == 'tensorflow2'

# OpenVINO™ Development Tools 

Intel® Distribution of OpenVINO™ toolkit is an open-source toolkit for optimizing and deploying AI inference. It can be used to develop applications and solutions based on deep learning tasks, such as: emulation of human vision, automatic speech recognition, natural language processing, recommendation systems, etc. It provides high-performance and rich deployment options, from edge to cloud.

OpenVINO™ Development Tools enables you to download models from Open Model Zoo, convert your own models to OpenVINO IR, as well as optimize and tune pre-trained deep learning models. See [What's in the Package](#whats-in-the-package) for more information.

## System Requirements

Before you start the installation, check the supported operating systems and required Python* versions. The complete list of supported hardware is available in the [System Requirements](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/system-requirements.html).

**C++ libraries** are also required for the installation on Windows*. To install that, you can [download the Visual Studio Redistributable file (.exe)](https://aka.ms/vs/17/release/vc_redist.x64.exe).

> **NOTE**: This package can be installed on other versions of macOS, Linux and Windows, but only the specific versions above are fully validated.

## Install the OpenVINO™ Development Tools Package

There are two options to install OpenVINO Development Tools: installation into an existing environment with a deep learning framework used for model training or creation;
or installation in a new environment.

### Installation into an Existing Environment with the Source Deep Learning Framework

To install OpenVINO Development Tools (see the [What's in the Package](#whats-in-the-package) section of this article) into an existing environment
with the source deep learning framework used for model training or creation, run the following command:
```
pip install openvino-dev
```

### Installation in a New Environment

If you do not have an environment with the source deep learning framework for the input model or you encounter any compatibility issues between OpenVINO and your version of deep learning framework,
you may install OpenVINO Development Tools with validated versions of frameworks into a new environment. 

#### Step 1. Set Up Python Virtual Environment

Use a virtual environment to avoid dependency conflicts. 

To create a virtual environment, use the following commands:

On Windows:
```sh
python -m venv openvino_env
```

On Linux and macOS:
```sh
python3 -m venv openvino_env
```

> **NOTE**: On Linux and macOS, you may need to [install pip](https://pip.pypa.io/en/stable/installation/). For example, on Ubuntu execute the following command to get pip installed: `sudo apt install python3-venv python3-pip`.

#### Step 2. Activate Virtual Environment

On Linux and macOS:
```sh
source openvino_env/bin/activate
```
On Windows:
```sh
openvino_env\Scripts\activate
```

#### Step 3. Set Up and Update PIP to the Highest Version

Run the command below:
```sh
python -m pip install --upgrade pip
```

#### Step 4. Install the Package

Use the following command:
```sh
pip install openvino-dev[extras]
```
 where `extras` is the source deep learning framework for the input model and is one or more of the following values separated with "," : 

| Extras Value                    | DL Framework                                                                     |
| :-------------------------------| :------------------------------------------------------------------------------- |
| caffe                           |   [Caffe*](https://caffe.berkeleyvision.org/)                                    |
| kaldi                           |   [Kaldi*](https://github.com/kaldi-asr/kaldi)                                   |
| mxnet                           |   [Apache MXNet*](https://mxnet.apache.org/)                                     |
| onnx                            |   [ONNX*](https://github.com/microsoft/onnxruntime/)                             |
| pytorch                         |   [PyTorch*](https://pytorch.org/)                                               |
| tensorflow                      |   [TensorFlow* 1.x](https://www.tensorflow.org/versions#tensorflow_1)            |
| tensorflow2                     |   [TensorFlow* 2.x](https://www.tensorflow.org/versions#tensorflow_2)            |

For example, to install and configure the components for working with TensorFlow 2.x and ONNX models, use the following command:
   ```sh
   pip install openvino-dev[tensorflow2,onnx]
   ```
> **NOTE**: Model conversion API support for TensorFlow 1.x environment has been deprecated. Use TensorFlow 2.x environment to convert both TensorFlow 1.x and 2.x models.

> **NOTE**: On macOS, you may need to enclose the package name in quotes: `pip install "openvino-dev[extras]"`.

## How to Verify that the Package Is Installed

- To verify that the **developer package** is properly installed, run the command below (this may take a few seconds):
   ```sh
   mo -h
   ```
   You will see the help message for ``mo`` if installation finished successfully.

- To verify that OpenVINO Runtime from the **runtime package** is available, run the command below:
   ```sh
   python -c "from openvino.runtime import Core; print(Core().available_devices)"
   ```
   If installation was successful, you will see a list of available devices.

<a id="whats-in-the-package"></a>

## What's in the Package?

> **NOTE**: The openvino-dev package installs [OpenVINO™ Runtime](https://pypi.org/project/openvino) as a dependency, which is the engine that runs the deep learning model and includes a set of libraries for an easy inference integration into your applications.  

**In addition, the openvino-dev package installs the following components by default:**

| Component        | Console Script                                                                   | Description                                                                                                                                                                                                                                                                                                   |  
|------------------|---------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Model conversion API](https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) | `mo` |**Model Optimizer** imports, converts, and optimizes models that were trained in popular frameworks to a format usable by OpenVINO components. <br>Supported frameworks include Caffe\*, TensorFlow\*, MXNet\*, PaddlePaddle\*, and ONNX\*.                                               |
| [Benchmark Tool](https://docs.openvino.ai/2023.0/openvino_inference_engine_tools_benchmark_tool_README.html)| `benchmark_app` | **Benchmark Application** allows you to estimate deep learning inference performance on supported devices for synchronous and asynchronous modes.                                              |
| [Accuracy Checker](https://docs.openvino.ai/2023.0/omz_tools_accuracy_checker.html) and <br> [Annotation Converter](https://docs.openvino.ai/2023.0/omz_tools_accuracy_checker_annotation_converters.html) | `accuracy_check` <br> `convert_annotation` |**Accuracy Checker**  is a deep learning accuracy validation tool that allows you to collect accuracy metrics against popular datasets. The main advantages of the tool are the flexibility of configuration and a set of supported datasets, preprocessing, postprocessing, and metrics. <br> **Annotation Converter** is a utility that prepares datasets for evaluation with Accuracy Checker.                                             |
| [Post-Training Optimization Tool](https://docs.openvino.ai/2023.0/pot_introduction.html)| `pot` |**Post-Training Optimization Tool** allows you to optimize trained models with advanced capabilities, such as quantization and low-precision optimizations, without the need to retrain or fine-tune models.                                            |
| [Model Downloader and other Open Model Zoo tools](https://docs.openvino.ai/2023.0/omz_tools_downloader.html)| `omz_downloader` <br> `omz_converter` <br> `omz_quantizer` <br> `omz_info_dumper`| **Model Downloader** is a tool for getting access to the collection of high-quality and extremely fast pre-trained deep learning [public](@ref omz_models_group_public) and [Intel](@ref omz_models_group_intel)-trained models. These free pre-trained models can be used to speed up the development and production deployment process without training your own models. The tool downloads model files from online sources and, if necessary, patches them to make them more usable with Model Optimizer. A number of additional tools are also provided to automate the process of working with downloaded models:<br> **Model Converter** is a tool for converting Open Model Zoo models that are stored in an original deep learning framework format into the OpenVINO Intermediate Representation (IR) using Model Optimizer. <br> **Model Quantizer** is a tool for automatic quantization of full-precision models in the IR format into low-precision versions using the Post-Training Optimization Tool. <br> **Model Information Dumper** is a helper utility for dumping information about the models to a stable, machine-readable format.                                          |      


## Troubleshooting

For general troubleshooting steps and issues, see [Troubleshooting Guide for OpenVINO Installation](https://docs.openvino.ai/2023.0/openvino_docs_get_started_guide_troubleshooting.html). The following sections also provide explanations to several error messages. 

### Errors with Installing via PIP for Users in China

Users in China might encounter errors while downloading sources via PIP during OpenVINO™ installation. To resolve the issues, try the following solution:
   
* Add the download source using the ``-i`` parameter with the Python ``pip`` command. For example: 

   ``` sh
   pip install openvino-dev -i https://mirrors.aliyun.com/pypi/simple/
   ```
   Use the ``--trusted-host`` parameter if the URL above is ``http`` instead of ``https``.
   You can also run the following command to install openvino-dev with specific frameworks. For example:
   
   ```
   pip install openvino-dev[tensorflow2] -i https://mirrors.aliyun.com/pypi/simple/
   ```

### zsh: no matches found : openvino-dev[...]

If you use zsh (Z shell) interpreter, that is the default shell for macOS starting with version 10.15 (Catalina), you may encounter the following error while installing `openvino-dev` package with extras:

```sh
pip install openvino-dev[tensorflow2,mxnet,caffe]
zsh: no matches found: openvino-dev[tensorflow2,mxnet,caffe]
```

By default zsh interprets square brackets as an expression for pattern matching. To resolve this issue, you need to escape the command with quotes: 

```sh
pip install 'openvino-dev[tensorflow2,mxnet,caffe]'
```

To avoid such issues you can also disable globbing for PIP commands by defining an alias in `~/.zshrc` file:

```sh
alias pip='noglob pip'
```

### ERROR:root:Could not find the Inference Engine or nGraph Python API.

On Windows*, some libraries are necessary to run OpenVINO. To resolve this issue, install the [C++ redistributable (.exe)](https://aka.ms/vs/17/release/vc_redist.x64.exe). You can also view a full download list on the [official support page](https://docs.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist).

### ImportError: libpython3.7m.so.1.0: cannot open shared object file: No such file or directory

To resolve missing external dependency on Ubuntu* 18.04, execute the following command:
```sh
sudo apt-get install libpython3.7
```

## Additional Resources

- [Intel® Distribution of OpenVINO™ toolkit](https://software.intel.com/en-us/openvino-toolkit)
- [OpenVINO™ Documentation](https://docs.openvino.ai/)
- [OpenVINO™ Notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- [OpenVINO Installation Selector Tool](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/download.html)

Copyright © 2018-2023 Intel Corporation
> **LEGAL NOTICE**: Your use of this software and any required dependent software (the
“Software Package”) is subject to the terms and conditions of the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0.html) for the Software Package, which may also include notices, disclaimers, or
license terms for third party or open source software included in or with the Software Package, and your use indicates your acceptance of all such terms. Please refer to the “third-party-programs.txt” or other similarly-named text file included with the Software Package for additional details.

>Intel is committed to the respect of human rights and avoiding complicity in human rights abuses, a policy reflected in the [Intel Global Human Rights Principles](https://www.intel.com/content/www/us/en/policy/policy-human-rights.html). Accordingly, by accessing the Intel material on this platform you agree that you will not use the material in a product or application that causes or contributes to a violation of an internationally recognized human right.
